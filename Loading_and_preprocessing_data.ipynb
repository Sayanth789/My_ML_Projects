{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814b9738",
   "metadata": {},
   "source": [
    "\n",
    "Exercises\n",
    "9.a.\n",
    "Exercise: Load the Fashion MNIST dataset; split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label. Note: for large images, you could use tf.io.encode_jpeg() instead. This would save a lot of space, but it would lose a bit of image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64544de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_valid , X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3304a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f1b27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "677b2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.train import  Example, Features, Feature, BytesList, Int64List\n",
    "\n",
    "\n",
    "\n",
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    # image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\":Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\":Feature(int64_list=Int64List(value=[label])),\n",
    "\n",
    "            }))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abff4f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"label\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 9\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"image\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\rI\\000\\000\\001\\004\\000\\000\\000\\000\\001\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000$\\210\\177>6\\000\\000\\000\\001\\003\\004\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000f\\314\\260\\206\\220{\\027\\000\\000\\000\\000\\014\\n\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\233\\354\\317\\262k\\234\\241m@\\027M\\202H\\017\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000E\\317\\337\\332\\330\\330\\243\\177yz\\222\\215X\\254B\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\001\\001\\000\\310\\350\\350\\351\\345\\337\\337\\327\\325\\244\\177{\\304\\345\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\267\\341\\330\\337\\344\\353\\343\\340\\336\\340\\335\\337\\365\\255\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\301\\344\\332\\325\\306\\264\\324\\322\\323\\325\\337\\334\\363\\312\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\003\\000\\014\\333\\334\\324\\332\\300\\251\\343\\320\\332\\340\\324\\342\\305\\3214\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000c\\364\\336\\334\\332\\313\\306\\335\\327\\325\\336\\334\\365w\\2478\\000\\000\\000\\000\\000\\000\\000\\000\\000\\004\\000\\0007\\354\\344\\346\\344\\360\\350\\325\\332\\337\\352\\331\\331\\321\\\\000\\000\\000\\001\\004\\006\\007\\002\\000\\000\\000\\000\\000\\355\\342\\331\\337\\336\\333\\336\\335\\330\\337\\345\\327\\332\\377M\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000>\\221\\314\\344\\317\\325\\335\\332\\320\\323\\332\\340\\337\\333\\327\\340\\364\\237\\000\\000\\000\\000\\000\\022,Rk\\275\\344\\334\\336\\331\\342\\310\\315\\323\\346\\340\\352\\260\\274\\372\\370\\351\\356\\327\\000\\0009\\273\\320\\340\\335\\340\\320\\314\\326\\320\\321\\310\\237\\365\\301\\316\\337\\377\\377\\335\\352\\335\\323\\334\\350\\366\\000\\003\\312\\344\\340\\335\\323\\323\\326\\315\\315\\315\\334\\360P\\226\\377\\345\\335\\274\\232\\277\\322\\314\\321\\336\\344\\341\\000b\\351\\306\\322\\336\\345\\345\\352\\371\\334\\302\\327\\331\\361AIju\\250\\333\\335\\327\\331\\337\\337\\340\\345\\035K\\314\\324\\314\\301\\315\\323\\341\\330\\271\\305\\316\\306\\325\\360\\303\\343\\365\\357\\337\\332\\324\\321\\336\\334\\335\\346C0\\313\\267\\302\\325\\305\\271\\276\\302\\300\\312\\326\\333\\335\\334\\354\\341\\330\\307\\316\\272\\265\\261\\254\\265\\315\\316s\\000z\\333\\301\\263\\253\\267\\304\\314\\322\\325\\317\\323\\322\\310\\304\\302\\277\\303\\277\\306\\300\\260\\234\\247\\261\\322\\\\000\\000J\\275\\324\\277\\257\\254\\257\\265\\271\\274\\275\\274\\301\\306\\314\\321\\322\\322\\323\\274\\274\\302\\300\\330\\252\\000\\002\\000\\000\\000B\\310\\336\\355\\357\\362\\366\\363\\364\\335\\334\\301\\277\\263\\266\\266\\265\\260\\246\\250c:\\000\\000\\000\\000\\000\\000\\000\\000\\000(=,H)#\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for image , label in valid_set.take(1):\n",
    "    print(create_example(image, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436bb37",
   "metadata": {},
   "source": [
    "The following function saves a given dataset to a set of TFRecord files. The examples are written to the files in a round-robin fashion. To do this, we enumerate all the examples using the dataset.enumerate() method, and we compute index % n_shards to decide which file to write to. We use the standard contextlib.ExitStack class to make sure that all writers are properly closed whether or not an I/O error occurs while writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1309e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def write_tfrecords(name, dataset, n_shards=10):\n",
    "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
    "            for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "                   for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "        return paths        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "798ae584",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n",
    "valid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\n",
    "test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c009c",
   "metadata": {},
   "source": [
    "Exercise:b.\n",
    " Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56b48823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None, n_parse_threads=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40add9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(valid_filepaths)\n",
    "test_set = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a56b5c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for X, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f72bb061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_8528\\3593870598.py:12: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  standardization = Standardization(input_shape=[28, 28])\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "\n",
    "standardization = Standardization(input_shape=[28, 28])\n",
    "# or perhaps soon\n",
    "# standardization = keras.layers.Normalization()\n",
    "\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image)        \n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
    "                              axis=0).astype(np.float32)\n",
    "standardization.adapt(sample_images)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    standardization, \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c99ea662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   1715/Unknown \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.8034 - loss: 1732.3492"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\Jupyter Projects\\env\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.8035 - loss: 1740.0853 - val_accuracy: 0.8586 - val_loss: 36012.7148\n",
      "Epoch 2/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.8782 - loss: 2310.7104 - val_accuracy: 0.8722 - val_loss: 53385.5312\n",
      "Epoch 3/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8901 - loss: 2934.2427 - val_accuracy: 0.8844 - val_loss: 3794.0176\n",
      "Epoch 4/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9000 - loss: 1478.8806 - val_accuracy: 0.8764 - val_loss: 4503.2476\n",
      "Epoch 5/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9067 - loss: 771.8546 - val_accuracy: 0.8856 - val_loss: 4012.1980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e96185e660>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "logs = os.path.join(os.curdir, \"my_logs\",\n",
    "                    \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set, callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0681f29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e4299a75ae178b2e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e4299a75ae178b2e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
