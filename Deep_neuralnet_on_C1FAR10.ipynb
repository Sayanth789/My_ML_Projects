{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d601d42",
   "metadata": {},
   "source": [
    "# Build a DNN with 20 hidden layer of 100 neurons each.Use He initialization and ELU activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88f7dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\Desktop\\Jupyter Projects\\env\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\Jupyter Projects\\env\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorboard\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "np.random.seed(42)\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6225bac",
   "metadata": {},
   "source": [
    "Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it as: keras.datasets.cifar10.Load_data(). The dataset is composed of 60000 32 x 32 pixel color images(50000 for training and 10000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the models's architecture or hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "132aa029",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6845e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the Nadam optimizer with a learning rate of 5e-5, it is obtained after\n",
    "# some trial and error, trying learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and comparing their learning curves for 10 epochs each (using the TensorBoard callback, below).  \n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569093e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the CIFAR10 dataset. We also want to use Early stopping so we need a validation set.\n",
    "# We use first 5000 images of the original training set as the  validation set.\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_cifar10_batch(batch_filename):\n",
    "    with open(batch_filename, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='latin1')\n",
    "        data = batch['data']\n",
    "        labels = batch['labels']\n",
    "        data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        return data, np.array(labels)\n",
    "\n",
    "def load_cifar10(cifar10_dir):\n",
    "    X_train_full, y_train_full = [], []\n",
    "    for i in range(1, 6):\n",
    "        batch_path = os.path.join(cifar10_dir, f'data_batch_{i}')\n",
    "        data, labels = load_cifar10_batch(batch_path)\n",
    "        X_train_full.append(data)\n",
    "        y_train_full.append(labels)\n",
    "    X_train_full = np.concatenate(X_train_full)\n",
    "    y_train_full = np.concatenate(y_train_full)\n",
    "\n",
    "    X_test, y_test = load_cifar10_batch(os.path.join(cifar10_dir, 'test_batch'))\n",
    "    return (X_train_full, y_train_full), (X_test, y_test)\n",
    "\n",
    "# 汨 Change this to where you extracted the files\n",
    "cifar10_path = r\"C:\\Users\\ADMIN\\Downloads\\cifar-10-python\\cifar-10-batches-py\"\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = load_cifar10(cifar10_path)\n",
    "\n",
    "# Same split logic as before\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461936e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create callbacks we need and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f01403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # Increment every time you train the model.\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tesnsorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tesnsorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7174c2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7685a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1284 - loss: 10.9341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 15ms/step - accuracy: 0.1285 - loss: 10.9157 - val_accuracy: 0.2226 - val_loss: 2.1710\n",
      "Epoch 2/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2247 - loss: 2.1210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 15ms/step - accuracy: 0.2247 - loss: 2.1209 - val_accuracy: 0.2668 - val_loss: 2.0167\n",
      "Epoch 3/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2699 - loss: 1.9793"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 13ms/step - accuracy: 0.2699 - loss: 1.9792 - val_accuracy: 0.2952 - val_loss: 1.9232\n",
      "Epoch 4/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3067 - loss: 1.9055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.3068 - loss: 1.9054 - val_accuracy: 0.3236 - val_loss: 1.8733\n",
      "Epoch 5/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3288 - loss: 1.8401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.3288 - loss: 1.8400 - val_accuracy: 0.3678 - val_loss: 1.7711\n",
      "Epoch 6/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3583 - loss: 1.7776"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.3583 - loss: 1.7776 - val_accuracy: 0.3650 - val_loss: 1.7543\n",
      "Epoch 7/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3757 - loss: 1.7285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.3757 - loss: 1.7285 - val_accuracy: 0.3818 - val_loss: 1.7191\n",
      "Epoch 8/100\n",
      "\u001b[1m1406/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3909 - loss: 1.6867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.3909 - loss: 1.6867 - val_accuracy: 0.4098 - val_loss: 1.6636\n",
      "Epoch 9/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4103 - loss: 1.6497"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4103 - loss: 1.6497 - val_accuracy: 0.4080 - val_loss: 1.6562\n",
      "Epoch 10/100\n",
      "\u001b[1m1400/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4169 - loss: 1.6214"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4169 - loss: 1.6214 - val_accuracy: 0.4226 - val_loss: 1.6341\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4282 - loss: 1.5965 - val_accuracy: 0.4146 - val_loss: 1.6391\n",
      "Epoch 12/100\n",
      "\u001b[1m1402/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4372 - loss: 1.5751"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4372 - loss: 1.5751 - val_accuracy: 0.4108 - val_loss: 1.6278\n",
      "Epoch 13/100\n",
      "\u001b[1m1405/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4434 - loss: 1.5538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4434 - loss: 1.5537 - val_accuracy: 0.4278 - val_loss: 1.6004\n",
      "Epoch 14/100\n",
      "\u001b[1m1403/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4500 - loss: 1.5367"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4500 - loss: 1.5367 - val_accuracy: 0.4266 - val_loss: 1.5985\n",
      "Epoch 15/100\n",
      "\u001b[1m1402/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4553 - loss: 1.5207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4553 - loss: 1.5207 - val_accuracy: 0.4320 - val_loss: 1.5866\n",
      "Epoch 16/100\n",
      "\u001b[1m1403/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4570 - loss: 1.5023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4570 - loss: 1.5023 - val_accuracy: 0.4382 - val_loss: 1.5710\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4631 - loss: 1.4906 - val_accuracy: 0.4352 - val_loss: 1.5827\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4697 - loss: 1.4736 - val_accuracy: 0.4394 - val_loss: 1.5824\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4760 - loss: 1.4567 - val_accuracy: 0.4394 - val_loss: 1.5821\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4819 - loss: 1.4436 - val_accuracy: 0.4330 - val_loss: 1.5831\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4861 - loss: 1.4326 - val_accuracy: 0.4394 - val_loss: 1.5820\n",
      "Epoch 22/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4901 - loss: 1.4206 - val_accuracy: 0.4428 - val_loss: 1.5983\n",
      "Epoch 23/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4961 - loss: 1.4093 - val_accuracy: 0.4422 - val_loss: 1.5981\n",
      "Epoch 24/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4990 - loss: 1.3935 - val_accuracy: 0.4398 - val_loss: 1.5992\n",
      "Epoch 25/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.5048 - loss: 1.3838 - val_accuracy: 0.4388 - val_loss: 1.6041\n",
      "Epoch 26/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.5093 - loss: 1.3729 - val_accuracy: 0.4420 - val_loss: 1.5963\n",
      "Epoch 27/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.5114 - loss: 1.3631 - val_accuracy: 0.4434 - val_loss: 1.6103\n",
      "Epoch 28/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.5153 - loss: 1.3523 - val_accuracy: 0.4470 - val_loss: 1.6021\n",
      "Epoch 29/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5204 - loss: 1.3421 - val_accuracy: 0.4442 - val_loss: 1.6233\n",
      "Epoch 30/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.5218 - loss: 1.3321 - val_accuracy: 0.4452 - val_loss: 1.6206\n",
      "Epoch 31/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.5259 - loss: 1.3248 - val_accuracy: 0.4376 - val_loss: 1.6227\n",
      "Epoch 32/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5279 - loss: 1.3163 - val_accuracy: 0.4396 - val_loss: 1.6200\n",
      "Epoch 33/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.5327 - loss: 1.3048 - val_accuracy: 0.4450 - val_loss: 1.6261\n",
      "Epoch 34/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.5386 - loss: 1.2941 - val_accuracy: 0.4422 - val_loss: 1.6282\n",
      "Epoch 35/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5395 - loss: 1.2883 - val_accuracy: 0.4438 - val_loss: 1.6237\n",
      "Epoch 36/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5428 - loss: 1.2777 - val_accuracy: 0.4464 - val_loss: 1.6340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24af358a990>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cae52631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4412 - loss: 1.5728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5710365772247314, 0.4381999969482422]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847f516",
   "metadata": {},
   "source": [
    "# Now try adding Batch Normalization and compare the learning curves:Is it converging faster than before? Does it produce a better model? How does it affect\n",
    "# training speed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To answer this part, we use the same code as above except some changes:\n",
    "#  *) adding a BN layer after every Dense layer(before the activation fn), except for the output layer. Also added a BN layer \n",
    "# before the first hidden layer.\n",
    "\n",
    "# *) Changed the learning rate to 5e-4.\n",
    "# *) Renamed the run directions to run_bn_* and the model file name to my_cifar10_bn_model.h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb63604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\Desktop\\Jupyter Projects\\env\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\Desktop\\Jupyter Projects\\env\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2976 - loss: 1.9675"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 11ms/step - accuracy: 0.2976 - loss: 1.9674 - val_accuracy: 0.3948 - val_loss: 1.6898\n",
      "Epoch 2/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4038 - loss: 1.6828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.4038 - loss: 1.6828 - val_accuracy: 0.4334 - val_loss: 1.6006\n",
      "Epoch 3/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4383 - loss: 1.5886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4383 - loss: 1.5886 - val_accuracy: 0.4464 - val_loss: 1.5590\n",
      "Epoch 4/100\n",
      "\u001b[1m1402/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4616 - loss: 1.5180"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.4616 - loss: 1.5180 - val_accuracy: 0.4550 - val_loss: 1.5346\n",
      "Epoch 5/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4873 - loss: 1.4559"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4873 - loss: 1.4559 - val_accuracy: 0.4648 - val_loss: 1.5184\n",
      "Epoch 6/100\n",
      "\u001b[1m1403/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5068 - loss: 1.4000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.5068 - loss: 1.4000 - val_accuracy: 0.4698 - val_loss: 1.5043\n",
      "Epoch 7/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.5299 - loss: 1.3455 - val_accuracy: 0.4636 - val_loss: 1.5246\n",
      "Epoch 8/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.5478 - loss: 1.2971 - val_accuracy: 0.4720 - val_loss: 1.5216\n",
      "Epoch 9/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.5661 - loss: 1.2508 - val_accuracy: 0.4642 - val_loss: 1.5659\n",
      "Epoch 10/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.5830 - loss: 1.2124 - val_accuracy: 0.4636 - val_loss: 1.5907\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5931 - loss: 1.1734 - val_accuracy: 0.4640 - val_loss: 1.6209\n",
      "Epoch 12/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.6074 - loss: 1.1387 - val_accuracy: 0.4562 - val_loss: 1.6718\n",
      "Epoch 13/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.6185 - loss: 1.1097 - val_accuracy: 0.4550 - val_loss: 1.6794\n",
      "Epoch 14/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6315 - loss: 1.0714 - val_accuracy: 0.4588 - val_loss: 1.7093\n",
      "Epoch 15/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6418 - loss: 1.0443 - val_accuracy: 0.4526 - val_loss: 1.7377\n",
      "Epoch 16/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6517 - loss: 1.0141 - val_accuracy: 0.4592 - val_loss: 1.7501\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.6645 - loss: 0.9844 - val_accuracy: 0.4648 - val_loss: 1.7510\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.6706 - loss: 0.9618 - val_accuracy: 0.4606 - val_loss: 1.8050\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.6767 - loss: 0.9473 - val_accuracy: 0.4688 - val_loss: 1.8204\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.6886 - loss: 0.9135 - val_accuracy: 0.4594 - val_loss: 1.8688\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.6937 - loss: 0.8922 - val_accuracy: 0.4616 - val_loss: 1.8871\n",
      "Epoch 22/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.6986 - loss: 0.8686 - val_accuracy: 0.4646 - val_loss: 1.9384\n",
      "Epoch 23/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.7150 - loss: 0.8413 - val_accuracy: 0.4646 - val_loss: 1.9630\n",
      "Epoch 24/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.7182 - loss: 0.8231 - val_accuracy: 0.4580 - val_loss: 1.9989\n",
      "Epoch 25/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.7241 - loss: 0.8046 - val_accuracy: 0.4638 - val_loss: 2.0528\n",
      "Epoch 26/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 13ms/step - accuracy: 0.7294 - loss: 0.7957 - val_accuracy: 0.4640 - val_loss: 2.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4703 - loss: 1.5168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.504280686378479, 0.4697999954223633]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # Increment every time you train the model.\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tesnsorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tesnsorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3470f8",
   "metadata": {},
   "source": [
    "# Excr:\n",
    "# Try replacing the Batch Normalization with SELU, and make the necessary adustments to ensure that the network self-normalizes(i.e, standardize the input features, \n",
    "# use LeCun normal initialization, make sure the DDN contains only a sequence of dense layer, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6f545ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2787 - loss: 2.0281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 15ms/step - accuracy: 0.2787 - loss: 2.0280 - val_accuracy: 0.3432 - val_loss: 1.7810\n",
      "Epoch 2/100\n",
      "\u001b[1m1405/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3840 - loss: 1.7252"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.3840 - loss: 1.7251 - val_accuracy: 0.4098 - val_loss: 1.6911\n",
      "Epoch 3/100\n",
      "\u001b[1m1406/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4248 - loss: 1.6197"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.4248 - loss: 1.6196 - val_accuracy: 0.4366 - val_loss: 1.6203\n",
      "Epoch 4/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4533 - loss: 1.5477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 15ms/step - accuracy: 0.4533 - loss: 1.5477 - val_accuracy: 0.4486 - val_loss: 1.5609\n",
      "Epoch 5/100\n",
      "\u001b[1m1405/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4813 - loss: 1.4853"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.4813 - loss: 1.4853 - val_accuracy: 0.4678 - val_loss: 1.5541\n",
      "Epoch 6/100\n",
      "\u001b[1m1406/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 1.4356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.4356 - val_accuracy: 0.4834 - val_loss: 1.5361\n",
      "Epoch 7/100\n",
      "\u001b[1m1404/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5135 - loss: 1.3981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 15ms/step - accuracy: 0.5135 - loss: 1.3980 - val_accuracy: 0.4796 - val_loss: 1.5271\n",
      "Epoch 8/100\n",
      "\u001b[1m1403/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5307 - loss: 1.3541"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.5307 - loss: 1.3541 - val_accuracy: 0.4782 - val_loss: 1.5235\n",
      "Epoch 9/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 15ms/step - accuracy: 0.5400 - loss: 1.3218 - val_accuracy: 0.4958 - val_loss: 1.5269\n",
      "Epoch 10/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 13ms/step - accuracy: 0.5630 - loss: 1.2823 - val_accuracy: 0.4906 - val_loss: 1.5388\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.5680 - loss: 1.2585 - val_accuracy: 0.4904 - val_loss: 1.5259\n",
      "Epoch 12/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5779 - loss: 1.2311 - val_accuracy: 0.4932 - val_loss: 1.5506\n",
      "Epoch 13/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.5956 - loss: 1.1914 - val_accuracy: 0.4926 - val_loss: 1.5979\n",
      "Epoch 14/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6001 - loss: 1.1724 - val_accuracy: 0.4944 - val_loss: 1.5724\n",
      "Epoch 15/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.6069 - loss: 1.1536 - val_accuracy: 0.5004 - val_loss: 1.5842\n",
      "Epoch 16/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6164 - loss: 1.1333 - val_accuracy: 0.5028 - val_loss: 1.5690\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6200 - loss: 1.1234 - val_accuracy: 0.5076 - val_loss: 1.5669\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6276 - loss: 1.0974 - val_accuracy: 0.5112 - val_loss: 1.5569\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.6365 - loss: 1.0710 - val_accuracy: 0.5108 - val_loss: 1.5811\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6414 - loss: 1.0615 - val_accuracy: 0.5046 - val_loss: 1.6052\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6560 - loss: 1.0248 - val_accuracy: 0.5042 - val_loss: 1.6001\n",
      "Epoch 22/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6610 - loss: 1.0103 - val_accuracy: 0.5044 - val_loss: 1.6171\n",
      "Epoch 23/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.6655 - loss: 0.9939 - val_accuracy: 0.5006 - val_loss: 1.6612\n",
      "Epoch 24/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.6676 - loss: 0.9877 - val_accuracy: 0.5016 - val_loss: 1.6611\n",
      "Epoch 25/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.6066 - loss: 1.1707 - val_accuracy: 0.5022 - val_loss: 1.6072\n",
      "Epoch 26/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.6730 - loss: 0.9797 - val_accuracy: 0.5014 - val_loss: 1.6571\n",
      "Epoch 27/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.6844 - loss: 0.9403 - val_accuracy: 0.4992 - val_loss: 1.7098\n",
      "Epoch 28/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.6901 - loss: 0.9355 - val_accuracy: 0.5084 - val_loss: 1.7541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4851 - loss: 1.5193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5234951972961426, 0.4781999886035919]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # Increment every time you train the model.\n",
    "tesnsorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tesnsorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks)\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c928e64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4851 - loss: 1.5193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5234951972961426, 0.4781999886035919]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70b1cf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1403/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2833 - loss: 2.0468"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.2834 - loss: 2.0462 - val_accuracy: 0.4018 - val_loss: 1.6902\n",
      "Epoch 2/100\n",
      "\u001b[1m1406/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3968 - loss: 1.6932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.3968 - loss: 1.6931 - val_accuracy: 0.4186 - val_loss: 1.6637\n",
      "Epoch 3/100\n",
      "\u001b[1m1406/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4359 - loss: 1.5940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.4359 - loss: 1.5940 - val_accuracy: 0.4448 - val_loss: 1.6289\n",
      "Epoch 4/100\n",
      "\u001b[1m1406/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4649 - loss: 1.5213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4649 - loss: 1.5213 - val_accuracy: 0.4566 - val_loss: 1.5827\n",
      "Epoch 5/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4918 - loss: 1.4548 - val_accuracy: 0.4754 - val_loss: 1.5849\n",
      "Epoch 6/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5114 - loss: 1.4097 - val_accuracy: 0.4754 - val_loss: 1.5891\n",
      "Epoch 7/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.5269 - loss: 1.3590 - val_accuracy: 0.4814 - val_loss: 1.6057\n",
      "Epoch 8/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.5437 - loss: 1.3199 - val_accuracy: 0.4854 - val_loss: 1.6456\n",
      "Epoch 9/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.5558 - loss: 1.2818 - val_accuracy: 0.4900 - val_loss: 1.5906\n",
      "Epoch 10/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.5680 - loss: 1.2516 - val_accuracy: 0.4836 - val_loss: 1.6308\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.5842 - loss: 1.2199 - val_accuracy: 0.4980 - val_loss: 1.6070\n",
      "Epoch 12/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.5930 - loss: 1.1912 - val_accuracy: 0.4812 - val_loss: 1.6868\n",
      "Epoch 13/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.5980 - loss: 1.1692 - val_accuracy: 0.4878 - val_loss: 1.7283\n",
      "Epoch 14/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6081 - loss: 1.1438 - val_accuracy: 0.4988 - val_loss: 1.7411\n",
      "Epoch 15/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6198 - loss: 1.1094 - val_accuracy: 0.5070 - val_loss: 1.7690\n",
      "Epoch 16/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6258 - loss: 1.0919 - val_accuracy: 0.4980 - val_loss: 1.8004\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6373 - loss: 1.0687 - val_accuracy: 0.4942 - val_loss: 1.7386\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.6433 - loss: 1.0480 - val_accuracy: 0.4940 - val_loss: 1.8336\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.6473 - loss: 1.0350 - val_accuracy: 0.4968 - val_loss: 1.8544\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6618 - loss: 1.0060 - val_accuracy: 0.4906 - val_loss: 1.8437\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6597 - loss: 1.0106 - val_accuracy: 0.4986 - val_loss: 1.8646\n",
      "Epoch 22/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6742 - loss: 0.9702 - val_accuracy: 0.5000 - val_loss: 1.8690\n",
      "Epoch 23/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.6751 - loss: 0.9636 - val_accuracy: 0.5042 - val_loss: 1.9068\n",
      "Epoch 24/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6775 - loss: 0.9621 - val_accuracy: 0.4986 - val_loss: 1.8567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4589 - loss: 1.5697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5826939344406128, 0.45660001039505005]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try regularizing the model with alpha dropout.Then, without retraining your model\n",
    "# see if you can achieve  better accuracy using MC dropout.\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # Increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar1_logs\", \"run_alpha_dropout{:03d}\".format(run_index))\n",
    "tesnsorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tesnsorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97e5f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MC Dropout \n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3987c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is identical to the one that just trained (with the same weights),but with MCAlphaDropout dropout layers instead of AlphaDropout layers.\n",
    "\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b36e4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we add a couple of utility fns: The first will run the model many times (10 by default) and it will return the mean predicted class probabilities.\n",
    "# The second will use these mean probabilites to predict the most likely class for each instance:\n",
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for smaple in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9ec57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.4566)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we make predictions for all instance in the validation set, and compute the accuracy.\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excer: Retain your model using 1cycle scheduling and see if it improves training speed and model accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5d451d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\Jupyter Projects\\env\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07505781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))    \n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fec74495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\Jupyter Projects\\env\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6294dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None, last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n",
    "\n",
    "    def on_batch(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0,self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate)\n",
    "\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate) \n",
    "        \n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e097ede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.2747 - loss: 2.0667 - val_accuracy: 0.3950 - val_loss: 1.7210\n",
      "Epoch 2/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4063 - loss: 1.6648 - val_accuracy: 0.4148 - val_loss: 1.6822\n",
      "Epoch 3/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.4489 - loss: 1.5504 - val_accuracy: 0.4284 - val_loss: 1.6815\n",
      "Epoch 4/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.4782 - loss: 1.4694 - val_accuracy: 0.4320 - val_loss: 1.6574\n",
      "Epoch 5/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5002 - loss: 1.4055 - val_accuracy: 0.4470 - val_loss: 1.6633\n",
      "Epoch 6/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5196 - loss: 1.3483 - val_accuracy: 0.4642 - val_loss: 1.6678\n",
      "Epoch 7/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5367 - loss: 1.3006 - val_accuracy: 0.4768 - val_loss: 1.6176\n",
      "Epoch 8/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5562 - loss: 1.2529 - val_accuracy: 0.4676 - val_loss: 1.6935\n",
      "Epoch 9/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5679 - loss: 1.2156 - val_accuracy: 0.4854 - val_loss: 1.6656\n",
      "Epoch 10/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5819 - loss: 1.1721 - val_accuracy: 0.4820 - val_loss: 1.7007\n",
      "Epoch 11/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5965 - loss: 1.1400 - val_accuracy: 0.4828 - val_loss: 1.7415\n",
      "Epoch 12/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6080 - loss: 1.1063 - val_accuracy: 0.4794 - val_loss: 1.7704\n",
      "Epoch 13/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.6184 - loss: 1.0750 - val_accuracy: 0.4872 - val_loss: 1.8074\n",
      "Epoch 14/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6281 - loss: 1.0490 - val_accuracy: 0.4834 - val_loss: 1.8289\n",
      "Epoch 15/15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6370 - loss: 1.0238 - val_accuracy: 0.4716 - val_loss: 1.8948\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "batch_size =32\n",
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid), callbacks=[onecycle])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
